{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3affd2d7",
   "metadata": {},
   "source": [
    "Smurfette principle derivation, analysis of movie summaries, women are more often than not mentioned with a male character.\n",
    "\n",
    "Process the summary, and count how many times each character is mentioned, use n-grams (2-3 ?) to see when they are mentioned together.\n",
    "\n",
    "Maybe measure throughout the years ?\n",
    "\n",
    "Or measure with different n for n-grams, interactive plot ? (possible to do both yearly + variable n too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "03a7f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import string\n",
    "import os\n",
    "import csv\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "#Punctuations from python string.punctuation without the dot ('.'), additional punctuation added to remove artifacts\n",
    "punctuation = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '/', ':',\n",
    " ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~'] + ['``', '\\'\\'', '\\'s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c93256f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 450669\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia movie ID</th>\n",
       "      <th>Freebase movie ID</th>\n",
       "      <th>Movie release date</th>\n",
       "      <th>Character name</th>\n",
       "      <th>Actor date of birth</th>\n",
       "      <th>Actor gender</th>\n",
       "      <th>Actor height (in meters)</th>\n",
       "      <th>Actor ethnicity (Freebase ID)</th>\n",
       "      <th>Actor name</th>\n",
       "      <th>Actor age at movie release</th>\n",
       "      <th>Freebase character/actor map ID</th>\n",
       "      <th>Freebase character ID</th>\n",
       "      <th>Freebase actor ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>975900</td>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>2001-08-24</td>\n",
       "      <td>Akooshay</td>\n",
       "      <td>1958-08-26</td>\n",
       "      <td>F</td>\n",
       "      <td>1.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wanda De Jesus</td>\n",
       "      <td>42.0</td>\n",
       "      <td>/m/0bgchxw</td>\n",
       "      <td>/m/0bgcj3x</td>\n",
       "      <td>/m/03wcfv7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>975900</td>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>2001-08-24</td>\n",
       "      <td>Lieutenant Melanie Ballard</td>\n",
       "      <td>1974-08-15</td>\n",
       "      <td>F</td>\n",
       "      <td>1.78</td>\n",
       "      <td>/m/044038p</td>\n",
       "      <td>Natasha Henstridge</td>\n",
       "      <td>27.0</td>\n",
       "      <td>/m/0jys3m</td>\n",
       "      <td>/m/0bgchn4</td>\n",
       "      <td>/m/0346l4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Wikipedia movie ID Freebase movie ID Movie release date  \\\n",
       "0              975900         /m/03vyhn         2001-08-24   \n",
       "1              975900         /m/03vyhn         2001-08-24   \n",
       "\n",
       "               Character name Actor date of birth Actor gender  \\\n",
       "0                    Akooshay          1958-08-26            F   \n",
       "1  Lieutenant Melanie Ballard          1974-08-15            F   \n",
       "\n",
       "   Actor height (in meters) Actor ethnicity (Freebase ID)          Actor name  \\\n",
       "0                      1.62                           NaN      Wanda De Jesus   \n",
       "1                      1.78                    /m/044038p  Natasha Henstridge   \n",
       "\n",
       "   Actor age at movie release Freebase character/actor map ID  \\\n",
       "0                        42.0                      /m/0bgchxw   \n",
       "1                        27.0                       /m/0jys3m   \n",
       "\n",
       "  Freebase character ID Freebase actor ID  \n",
       "0            /m/0bgcj3x        /m/03wcfv7  \n",
       "1            /m/0bgchn4         /m/0346l4  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Character metadata\n",
    "char_md_cols = ['Wikipedia movie ID',\n",
    "'Freebase movie ID',\n",
    "'Movie release date',\n",
    "'Character name',\n",
    "'Actor date of birth',\n",
    "'Actor gender',\n",
    "'Actor height (in meters)',\n",
    "'Actor ethnicity (Freebase ID)',\n",
    "'Actor name',\n",
    "'Actor age at movie release',\n",
    "'Freebase character/actor map ID',\n",
    "'Freebase character ID',\n",
    "'Freebase actor ID',\n",
    "]\n",
    "char_md = pd.read_csv('data/character.metadata.tsv', sep='\\t', names=char_md_cols)\n",
    "print('Number of characters:', char_md.shape[0])\n",
    "char_md.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "73ca4fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_gender = char_md[['Wikipedia movie ID', 'Character name', 'Actor gender']].dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "aba53f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_movie_IDS = os.listdir(\"./data/corenlp_plot_summaries/\")\n",
    "wikipedia_movie_IDS = [filename.rsplit('.', 2)[0] for filename in wikipedia_movie_IDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c6097228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42306"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wikipedia_movie_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1e373177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from xml.dom.minidom import parseString #, parse \n",
    "\n",
    "\n",
    "def open_parse_summary(wiki_id):\n",
    "    # open and read gzipped xml file\n",
    "    xml_file = gzip.open(\"./data/corenlp_plot_summaries/\" + wiki_id + \".xml.gz\")\n",
    "    document = parseString(xml_file.read())\n",
    "    \n",
    "    #Read all words\n",
    "    words = document.getElementsByTagName('word')\n",
    "    #lemmas = dom.getElementsByTagName('lemma')\n",
    "    \n",
    "    #Looks for the desired node type and extracts their values\n",
    "    words_list = [word.childNodes[0].nodeValue for word in words]\n",
    "    #lemmas_list = [lemma.childNodes[0].nodeValue for lemma in lemmas]\n",
    "    \n",
    "    return words_list#, lemmas_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b0547b76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26167604\n",
      "1/42306\n",
      "14075290\n",
      "2/42306\n",
      "24379585\n",
      "3/42306\n",
      "8683288\n",
      "4/42306\n",
      "18040146\n",
      "5/42306\n",
      "3113630\n",
      "6/42306\n",
      "15117612\n",
      "7/42306\n",
      "9404578\n",
      "8/42306\n",
      "21411112\n",
      "9/42306\n",
      "23553986\n",
      "10/42306\n",
      "9149984\n",
      "11/42306\n",
      "327582\n",
      "12/42306\n",
      "25375805\n",
      "13/42306\n",
      "5144523\n",
      "14/42306\n",
      "2817162\n",
      "15/42306\n",
      "24778429\n",
      "16/42306\n",
      "4298666\n",
      "17/42306\n",
      "8318237\n",
      "18/42306\n",
      "34383775\n",
      "19/42306\n",
      "15025803\n",
      "20/42306\n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN UNLESS NECESSARY !!!!!\n",
    "i = 0\n",
    "tot = len(wikipedia_movie_IDS)\n",
    "\n",
    "#for wiki_id in wikipedia_movie_IDS[0:20]:\n",
    "    print(wiki_id)\n",
    "    summary = list(open_parse_summary(wiki_id))\n",
    "    with open(\"./data/corenlp_summaries_words/\"+ wiki_id + \".txt\", 'w') as file_words:\n",
    "        for word in summary:\n",
    "            if word not in punctuation:\n",
    "                # write each item on a new line\n",
    "                file_words.write(\"%s\\n\" % word)\n",
    "    i += 1\n",
    "    print(str(i) + \"/\" + str(tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9abc3af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['26167604', '14075290', '24379585', '8683288', '18040146']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_movie_IDS[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "35b1dd49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia movie ID</th>\n",
       "      <th>Character name</th>\n",
       "      <th>Actor gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>975900</td>\n",
       "      <td>Akooshay</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>975900</td>\n",
       "      <td>Lieutenant Melanie Ballard</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>975900</td>\n",
       "      <td>Desolation Williams</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>975900</td>\n",
       "      <td>Sgt Jericho Butler</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>975900</td>\n",
       "      <td>Bashira Kincaid</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450658</th>\n",
       "      <td>913762</td>\n",
       "      <td>Lord Feff</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450659</th>\n",
       "      <td>913762</td>\n",
       "      <td>Additional Voices</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450661</th>\n",
       "      <td>913762</td>\n",
       "      <td>UN Spacy Commander</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450662</th>\n",
       "      <td>913762</td>\n",
       "      <td>Silvie Gena</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450664</th>\n",
       "      <td>913762</td>\n",
       "      <td>Elensh</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182639 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Wikipedia movie ID              Character name Actor gender\n",
       "0                   975900                    Akooshay            F\n",
       "1                   975900  Lieutenant Melanie Ballard            F\n",
       "2                   975900         Desolation Williams            M\n",
       "3                   975900          Sgt Jericho Butler            M\n",
       "4                   975900             Bashira Kincaid            F\n",
       "...                    ...                         ...          ...\n",
       "450658              913762                   Lord Feff            M\n",
       "450659              913762           Additional Voices            F\n",
       "450661              913762          UN Spacy Commander            M\n",
       "450662              913762                 Silvie Gena            F\n",
       "450664              913762                      Elensh            F\n",
       "\n",
       "[182639 rows x 3 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "34df9155",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3779/3926316479.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwiki_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwikipedia_movie_IDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_to_gender\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_to_gender\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Wikipedia movie ID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mempty_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3494\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3496\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3498\u001b[0m         \u001b[0;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3549\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3550\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3551\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3714\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3715\u001b[0m         \"\"\"\n\u001b[0;32m-> 3716\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3717\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3718\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3701\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3703\u001b[0;31m         new_data = self._mgr.take(\n\u001b[0m\u001b[1;32m   3704\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3705\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m         return self.reindex_indexer(\n\u001b[0m\u001b[1;32m    898\u001b[0m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    687\u001b[0m             )\n\u001b[1;32m    688\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m             new_blocks = [\n\u001b[0m\u001b[1;32m    690\u001b[0m                 blk.take_nd(\n\u001b[1;32m    691\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             new_blocks = [\n\u001b[0;32m--> 690\u001b[0;31m                 blk.take_nd(\n\u001b[0m\u001b[1;32m    691\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                     \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0mallow_fill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m         new_values = algos.take_nd(\n\u001b[0m\u001b[1;32m   1140\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "empty_count = 0\n",
    "avg_len = 0\n",
    "\n",
    "for wiki_id in wikipedia_movie_IDS:\n",
    "    test = char_to_gender[char_to_gender['Wikipedia movie ID'] == int(wiki_id)]\n",
    "    if(test.empty):\n",
    "        empty_count += 1\n",
    "    else:\n",
    "        avg_len += len(test)\n",
    "        \n",
    "avg_len /= (len(wikipedia_movie_IDS)-empty_count)\n",
    "\n",
    "print(\"Average of \" + str(avg_len) + \" actors per movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bd5b7ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords_and_general = spacy.lang.en.stop_words.STOP_WORDS\n",
    "#TODO: Add eventual additionnal words\n",
    "# Additional elements: man/wm/hb/wife cannot be processed as characters without more thorough language processing\n",
    "# Numbers: characters are sometimes described as for example 'Bartender 1', 'Bartender 2', these need to be avoided\n",
    "additional_elems = ['man', 'woman', 'husband', 'wife'] + ['0','1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "spacy_stopwords_and_general.update(additional_elems)\n",
    "\n",
    "def make_MF_repr(wiki_id):\n",
    "    with open(\"./data/corenlp_summaries_words/\"+ wiki_id + \".txt\", 'r') as summary_file:\n",
    "        #Read the summary\n",
    "        summary_text = summary_file.read().replace('\\n', ' ')\n",
    "        \n",
    "        #Get the characters from this movie\n",
    "        chars = char_to_gender[char_to_gender['Wikipedia movie ID'] == int(wiki_id)][['Character name', 'Actor gender']]\n",
    "        chars = dict(zip(chars['Character name'], chars['Actor gender']))\n",
    "        \n",
    "        #Remove punctuation from character names\n",
    "        for char_name in list(chars):\n",
    "            char_name_new = char_name\n",
    "            for pun in punctuation:\n",
    "                char_name_new = char_name_new.replace(pun, ' ')\n",
    "            chars[char_name_new] = chars.pop(char_name)\n",
    "        \n",
    "        #Remove characters which contain a stopword to keep (mostly) relevant characters (names).\n",
    "        for char_name in list(chars):\n",
    "            for word in char_name.split(' '):\n",
    "                if word.lower() in spacy_stopwords_and_general:\n",
    "                    chars.pop(char_name)\n",
    "                    break\n",
    "                    \n",
    "        #Replace character's names with M or F respectively\n",
    "        for char_name in list(chars):\n",
    "            summary_text = summary_text.replace(char_name, chars[char_name])\n",
    "            for word in char_name.split(' '):\n",
    "                summary_text = summary_text.replace(word, chars[char_name])\n",
    "                \n",
    "        # Save as final file format (M and F stay, other words get replaced by the number of words between two M/F)\n",
    "        # Each line is a new sentence\n",
    "        # For example, if the only character is named 'Bob' (man), 'Bob is cool be like Bob. I like Bob.' becomes:\n",
    "        # M4M\n",
    "        # 2M\n",
    "        #Check if chars list not empty\n",
    "        if(bool(chars)):\n",
    "            with open(\"./data/corenlp_summaries_encoded/\"+ wiki_id + \".txt\", 'w') as file_encoded:\n",
    "                count_non_character = 0\n",
    "                for word in summary_text.split(' '):\n",
    "                    if word == 'M' or word == 'F':\n",
    "                        if count_non_character != 0:\n",
    "                            file_encoded.write(\"%s\" % str(count_non_character))\n",
    "                        file_encoded.write(\"%s\" % word)\n",
    "                        count_non_character = 0\n",
    "                    elif word == '.':\n",
    "                        if count_non_character != 0:\n",
    "                            file_encoded.write(\"%s\" % str(count_non_character))\n",
    "                        file_encoded.write(\"\\n\")\n",
    "                        count_non_character = 0\n",
    "                    else:\n",
    "                        count_non_character += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ca1d07f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3493001002.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_3779/3493001002.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    make_MF_repr(wiki_id)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Do not rerun unless necessary\n",
    "#for wiki_id in wikipedia_movie_IDS[0:20]:\n",
    "    make_MF_repr(wiki_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fabb9b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the total occurence of M/F characters in the summary.\n",
    "def count_pure_M_F(wiki_id):\n",
    "    with open(\"./data/corenlp_summaries_encoded/\"+ wiki_id + \".txt\", 'r') as summary_file:\n",
    "        summary_str = summary_file.read()\n",
    "        return summary_str.count('M'), summary_str.count('F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "477482c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the occurence of M and F at a distance <= n. For ex. 'M2F' would give 1 for n=3 but 0 for n=2 or n=1\n",
    "# Not specifying n means taking the whole sentence\n",
    "def count_co_occ_M_F(wiki_id, n = 1000):\n",
    "    with open(\"./data/corenlp_summaries_encoded/\"+ wiki_id + \".txt\", 'r') as summary_file:\n",
    "        summary_str = summary_file.read()\n",
    "        co_occ_sum = {'M': 0, 'F': 0}\n",
    "        for line in summary_str.split('\\n'):\n",
    "            previous = ''\n",
    "            indic_hasbeencounted = 0\n",
    "            int_saver_M_F = {'M':0, 'F':0}\n",
    "            strint_saver = '0'\n",
    "            for word in line:\n",
    "                if word == 'M' or word == 'F':\n",
    "                    opposite_word = 'F' if word == 'M' else 'F'\n",
    "                    int_saver_M_F[opposite_word] += int(strint_saver)\n",
    "                    if word != previous and previous and int_saver_M_F[opposite_word] <= n-1:\n",
    "                        co_occ_sum[word] += 1\n",
    "                        int_saver_M_F[word] = 0\n",
    "                        if not indic_hasbeencounted:\n",
    "                            co_occ_sum[previous] += 1\n",
    "                        indic_hasbeencounted = 1\n",
    "                    else:\n",
    "                        indic_hasbeencounted = 0\n",
    "                    previous = word\n",
    "                    strint_saver = '0'\n",
    "                else:\n",
    "                    strint_saver += word\n",
    "        return co_occ_sum['M'], co_occ_sum['F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "004fab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the occurence of M and F at a distance <= n. For ex. 'M2F' would give 1 for n=3 but 0 for n=2 or n=1\n",
    "# Not specifying n means taking the whole sentence\n",
    "def count_co_occ_M_F(wiki_id, n = 1000):\n",
    "    with open(\"./data/corenlp_summaries_encoded/\"+ wiki_id + \".txt\", 'r') as summary_file:\n",
    "        summary_str = summary_file.read()\n",
    "        co_occ_sum = {'M': 0, 'F': 0}\n",
    "        for line in summary_str.split('\\n'):\n",
    "            distances = {'M': np.array([]), 'F': np.array([])}\n",
    "            indicators_counted = {'M': np.array([]), 'F': np.array([])}\n",
    "            strint_saver = '0'\n",
    "            for word in line:\n",
    "                if word == 'M' or word == 'F':\n",
    "                    \n",
    "                    # Gets the opposite word\n",
    "                    opposite_word = 'F' if word == 'M' else 'M'\n",
    "                    \n",
    "                    # Adds the current word in the distances and indicators\n",
    "                    distances[word] += int(strint_saver) + 1\n",
    "                    distances[word] = np.append(distances[word], 0)\n",
    "                    if not distances[opposite_word].size == 0:\n",
    "                        distances[opposite_word] += int(strint_saver) + 1\n",
    "                        \n",
    "                        # Gets the indices where the distance is < n and the word hasn't been counted in the sum\n",
    "                        indices_dist = np.where(distances[opposite_word] <= n)[0]\n",
    "                        indices_indic = np.where(indicators_counted[opposite_word] == 0)[0]\n",
    "                        valid_indices = np.intersect1d(indices_dist, indices_indic)\n",
    "                        \n",
    "                        # If there was an occurence of the opposite gender at a valid distance\n",
    "                        if indices_dist.size >= 1:\n",
    "                            \n",
    "                            # Increases the sum of the current word by 1 and indicates that occurence as counted\n",
    "                            co_occ_sum[word] += 1\n",
    "                            indicators_counted[word] = np.append(indicators_counted[word], 1)\n",
    "                            if indices_indic.size >= 1:\n",
    "                                \n",
    "                                # Increases the sum of the opposite word by the number of previous uncounted occurences\n",
    "                                # and indicates these occurences as counted\n",
    "                                co_occ_sum[opposite_word] += valid_indices.size\n",
    "                                indicators_counted[opposite_word][valid_indices] = 1\n",
    "                        else:\n",
    "                            \n",
    "                            # Indicate the current occurence as uncounted\n",
    "                            indicators_counted[word] = np.append(indicators_counted[word], 0)\n",
    "                    else:\n",
    "                        indicators_counted[word] = np.append(indicators_counted[word], 0)\n",
    "                        \n",
    "                    # Reset the strint_saver\n",
    "                    strint_saver = '0'\n",
    "                else:\n",
    "                    \n",
    "                    # Appends the strint saver with the current number\n",
    "                    strint_saver += word\n",
    "        return co_occ_sum['M'], co_occ_sum['F']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75090934",
   "metadata": {},
   "source": [
    "### Creating the final counts for graphing\n",
    "\n",
    "Given (start_n, end_n), computes the total M/F representations in the summaries.\n",
    "\n",
    "N represents the maximal distance between M and F to be counted as a co-occurence\n",
    "\n",
    "Saves the results in movie_MF_data.csv in the following form :\n",
    "\n",
    "[wiki_id_1, counts_1_start_n, ..., counts_1_end_n]  \n",
    "[...]  \n",
    "[wiki_id_final, counts_final_start_n, ..., counts_final_end_n]  \n",
    "[mean_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "666e3e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Makes the header for the csv\n",
    "def make_csv_header(start_n, end_n):\n",
    "    csv_header = ['wiki_id']\n",
    "    for n in range(start_n, end_n+1):\n",
    "        csv_header.extend(['total_M_'+str(n), 'total_F_'+str(n), 'c_o_M_'+str(n), 'c_o_F_'+str(n),\n",
    "                            'pure_M_'+str(n), 'pure_F_'+str(n),])\n",
    "    csv_header.extend(['total_M_sent', 'total_F_sent', 'c_o_M_sent', 'c_o_F_sent',\n",
    "                            'pure_M_sent', 'pure_F_sent'])\n",
    "    return csv_header\n",
    "\n",
    "def make_csv_linepart(wiki_id, n):\n",
    "    temp_line = []\n",
    "    \n",
    "    #Computes the total M, F\n",
    "    total_M, total_F = count_pure_M_F(wiki_id)\n",
    "    temp_line.extend([total_M, total_F])\n",
    "    \n",
    "    #Computes the co_occurences with given distance\n",
    "    co_occ_M, co_occ_F = count_co_occ_M_F(wiki_id, n)\n",
    "    temp_line.extend([co_occ_M, co_occ_F])\n",
    "    \n",
    "    #Computes the pure M, F\n",
    "    pure_M = total_M - co_occ_M\n",
    "    pure_F = total_F - co_occ_F\n",
    "    temp_line.extend([pure_M, pure_F])\n",
    "    \n",
    "    return temp_line\n",
    "\n",
    "#Makes the final counts for each movie\n",
    "def make_final_file(start_n, end_n):\n",
    "    csv_header = make_csv_header(start_n, end_n)\n",
    "    files = [filename.split('.txt')[0] for filename in os.listdir('./data/corenlp_summaries_encoded/')]\n",
    "    with open('./data/movie_MF_data_'+str(start_n)+'_'+str(end_n)+'.csv', 'w', encoding='UTF8', newline='') as final_file:\n",
    "        #Write the header\n",
    "        writer = csv.writer(final_file)\n",
    "        writer.writerow(csv_header)\n",
    "        \n",
    "        #Saving values for mean (6 per n and 6 for whole sentences)\n",
    "        mean_counts = np.zeros((end_n-start_n+2)*6 )\n",
    "        \n",
    "        #Write the counts for each file, for each possible n\n",
    "        for wiki_id in files:\n",
    "            final_file_line = [wiki_id]\n",
    "            \n",
    "            #Computes the counts for each n\n",
    "            for n in range(start_n, end_n+1):\n",
    "                final_file_line.extend(make_csv_linepart(wiki_id, n))\n",
    "                \n",
    "            #Computes the counts for whole sentences.\n",
    "            final_file_line.extend(make_csv_linepart(wiki_id, 1000))\n",
    "            mean_counts += np.array(final_file_line[1:])\n",
    "            writer.writerow(final_file_line)\n",
    "        mean_counts /= len(files)\n",
    "        means_line = ['means']\n",
    "        means_line.extend(mean_counts.tolist())\n",
    "        writer.writerow(means_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "244774ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_final_file(2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e9f76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
